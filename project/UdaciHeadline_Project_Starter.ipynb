{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro-header"
   },
   "source": [
    "# UdaciHeadline: LLM Inference Optimization Project\n",
    "\n",
    "## Project Introduction\n",
    "Large Language Models (LLMs) are transforming content creation, but deploying them efficiently remains a major hurdle. Imagine you're an ML Engineer at a bustling online news portal. Your key task? Automatically generating catchy headlines from article summaries using an LLM. The problem? The current inference process is sluggish, causing publication delays and driving up operational costs. In this project, UdaciHeadline, you'll step into this role and tackle this critical challenge head-on. Your mission is to accelerate the headline generation pipeline significantly by applying state-of-the-art LLM inference optimization techniques. Get ready to dive deep into practical optimization and deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "## Project Summary\n",
    "This project provides hands-on experience in optimizing the inference performance of a pre-trained Large Language Model (like Llama-3.2-1B) for news headline generation. You will bring together concepts of LLM architecture, optimization techniques, and deployment frameworks. Specifically, you will:\n",
    "\n",
    "1.  **Establish a baseline** inference pipeline and profile its performance.\n",
    "2.  Implement and evaluate architectural optimizations like **KV-caching**.\n",
    "3.  Apply model compression techniques like **quantization** and **pruning**.\n",
    "4.  Configure and benchmark **distributed inference** using Tensor and Pipeline Parallelism.\n",
    "5.  Apply advanced decoding mechanisms like **speculative decoding**.\n",
    "6.  Perform comprehensive **benchmarking and analysis** across all stages.\n",
    "7.  Produce a **final report** summarizing findings and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports-header"
   },
   "source": [
    "## Imports and Global Configuration\n",
    "\n",
    "Let's import the libraries we'll use throughout the project and define some constants like the model name and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.11.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "\u001b[33m  WARNING: The script evaluate-cli is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.6\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[33m  WARNING: The scripts hf, huggingface-cli and tiny-agents are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (4.65.0)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=2200809502d4ff2b8c0abe90b13943db8c88a13a2d52c0b0c76323502970003d\n",
      "  Stored in directory: /home/student/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: joblib, absl-py, nltk, rouge_score\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 joblib-1.5.2 nltk-3.9.2 rouge_score-0.1.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/student/.local/lib/python3.10/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/student/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "\u001b[33m  WARNING: The scripts accelerate, accelerate-config, accelerate-estimate-memory, accelerate-launch and accelerate-merge-weights are installed in '/home/student/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed accelerate-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install --upgrade transformers\n",
    "!pip install rouge_score\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "from time import time as get_time\n",
    "from pprint import pprint\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "#os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# ---- Constants ----\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-1B\"\n",
    "MAX_NEW_TOKENS = 50 # Max length for the generated headline\n",
    "\n",
    "#PROMPT = \\\n",
    "# print(f\"Prompt: \\\"{PROMPT}\\\"\")\n",
    "TARGET_LAYER_NAME_STR = \"model.layers.0.mlp.gate_proj\"\n",
    "\n",
    "# We will prune 50% of the weights in this layer\n",
    "PRUNING_AMOUNT = 0.5 \n",
    "NUM_PREDICTION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading-header"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We will use the \"News Category Dataset\" from Kaggle. The `kagglehub` library makes it easy to download and access. Your task is to implement the function to load and preprocess the data according to the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "data-loading-code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_news_dataset(path):\n",
    "    \"\"\"TODO: Implement the data loading and preprocessing logic here.\"\"\"\n",
    "    dataset = load_dataset(\"json\", data_files=path, split=\"train[:1000]\")\n",
    "    articles = [item[\"short_description\"] for item in dataset]\n",
    "\n",
    "    return dataset,articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline-header"
   },
   "source": [
    "# 2. Baseline Performance\n",
    "\n",
    "Before we can optimize, we need a starting point. Here, you'll establish the baseline performance of the `Llama-3.2-1B` model without any specific optimizations. We will measure latency, throughput, and the quality of the generated headlines using the ROUGE score.\n",
    "\n",
    "### Your Task: Implement the Evaluation Pipeline\n",
    "You need to implement the core functions for loading a model, generating a headline, and evaluating performance. These functions will be reused for every optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "baseline-helpers"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, quantization_config=None,device=\"cpu\"):\n",
    "    \"\"\"TODO: Implement the logic for loading a tokenizer and model.\"\"\"\n",
    "    \n",
    "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\",quantization_config=quantization_config).to(device)\n",
    "    print(\"Model loaded successfully and moved to device.\")\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def generate_headline(model, tokenizer,texts,device,max_length,use_cache=False):\n",
    "    \"\"\"TODO: Implement the headline generation and latency measurement logic.\"\"\"\n",
    "    headlines = []\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            start = get_time()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                use_cache=use_cache,  # Baseline: no KV caching\n",
    "                do_sample=False\n",
    "            )\n",
    "            end = get_time()\n",
    "            latencies.append(end - start)\n",
    "            headline = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            headlines.append(headline)\n",
    "    return headlines, latencies\n",
    "\n",
    "def report_metrics(times,section):#results, latencies, max_new_tokens):\n",
    "    \"\"\"TODO: Implement the logic for calculating and reporting all performance metrics.\"\"\"\n",
    "    avg_latency = sum(times) / len(times)\n",
    "    p99_latency = sorted(times)[int(0.99 * len(times))]\n",
    "    throughput = len(times) / sum(times)\n",
    "    gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    print(f\"Below are for: {section}s\")\n",
    "    print(f\"Avg Latency: {avg_latency:.3f}s\")\n",
    "    print(f\"P99 Latency: {p99_latency:.3f}s\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "    print(f\"Max GPU Memory: {gpu_memory:.2f} MB\")\n",
    "\n",
    "    pass\n",
    "\n",
    "def evaluate_model(dataset,generated,num_prediction):\n",
    "    \"\"\"TODO: Implement the model evaluation loop.\"\"\"\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    references = [item for item in dataset[:num_prediction]['headline']]\n",
    "    results = rouge.compute(predictions=generated, references=references)\n",
    "    print(\"ROUGE Scores:\", results)\n",
    "    \n",
    "    return results\n",
    "def clean(model,tokenizer):\n",
    "    # Clean up model from memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nCleaned up models and emptied CUDA cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name_str(model, module_name_str):\n",
    "    \"\"\"Gets a module from a model using its string name (e.g., 'model.layers.0.mlp.gate_proj').\"\"\"\n",
    "    names = module_name_str.split('.')\n",
    "    current_module = model\n",
    "    for name_part in names:\n",
    "        if hasattr(current_module, name_part):\n",
    "            current_module = getattr(current_module, name_part)\n",
    "        else:\n",
    "            try: # Handle numeric indices in ModuleLists\n",
    "                idx = int(name_part)\n",
    "                current_module = current_module[idx]\n",
    "            except (ValueError, TypeError, IndexError):\n",
    "                raise AttributeError(f\"Could not resolve name part '{name_part}' in '{module_name_str}'.\")\n",
    "    return current_module\n",
    "\n",
    "def calculate_sparsity(module, param_name='weight'):\n",
    "    \"\"\"Calculates sparsity of a named parameter in a module.\"\"\"\n",
    "    if hasattr(module, param_name):\n",
    "        param = getattr(module, param_name)\n",
    "        if param is not None:\n",
    "            return 100. * float(torch.sum(param == 0)) / float(param.nelement())\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 209527 examples [00:00, 326207.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "HF_TOKEN = ''\n",
    "login(token=HF_TOKEN)\n",
    "datasets, articles = load_news_dataset(\"../dataset/News_Category_Dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "baseline-eval"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for: Baselines\n",
      "Avg Latency: 4.185s\n",
      "P99 Latency: 4.795s\n",
      "Throughput: 0.24 samples/sec\n",
      "Max GPU Memory: 2372.02 MB\n",
      "ROUGE Scores: {'rouge1': 0.051078657630867316, 'rouge2': 0.0, 'rougeL': 0.04266694403994176, 'rougeLsum': 0.04434559579139584}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.051078657630867316,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.04266694403994176,\n",
       " 'rougeLsum': 0.04434559579139584}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Establish your baseline performance.\n",
    "\n",
    "dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "model_new,tokenizer = load_model(model_name=model_name,quantization_config = dtype,device=device)\n",
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_new, tokenizer, sample_texts, device,MAX_NEW_TOKENS)\n",
    "report_metrics(times,\"Baseline\")\n",
    "evaluate_model(datasets,generated,NUM_PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv-cache-header"
   },
   "source": [
    "# 3. Architectural Optimization: KV Caching\n",
    "\n",
    "**Your Task:** One of the most effective ways to speed up token generation is using a Key-Value (KV) cache. This avoids re-computing attention scores for tokens that are already part of the sequence. Enable the `use_cache` flag in the generation arguments and re-run the evaluation. Observe the impact on latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kv-cache-code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for: KV Cachings\n",
      "Avg Latency: 1.225s\n",
      "P99 Latency: 1.286s\n",
      "Throughput: 0.82 samples/sec\n",
      "Max GPU Memory: 2372.02 MB\n",
      "ROUGE Scores: {'rouge1': 0.058109756940126295, 'rouge2': 0.0, 'rougeL': 0.04490838618745595, 'rougeLsum': 0.0462616749415655}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.058109756940126295,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.04490838618745595,\n",
       " 'rougeLsum': 0.0462616749415655}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Evaluate the model with KV Caching enabled.\n",
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_new, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "report_metrics(times,\"KV Caching\")\n",
    "evaluate_model(datasets,generated,NUM_PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pruning-header"
   },
   "source": [
    "# 4. Model Compression: Pruning\n",
    "\n",
    "**Your Task:** Pruning removes redundant model weights, which can reduce model size and potentially speed up inference. Here, you will implement unstructured, magnitude-based pruning by creating a function that applies it to the model's linear layers and then evaluating the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pruning-code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accessing Target Layer: model.layers.0.mlp.gate_proj ---\n",
      "Successfully accessed target layer of type: <class 'torch.nn.modules.linear.Linear'>\n",
      "Sparsity of 'model.layers.0.mlp.gate_proj.weight' BEFORE pruning: 0.00%\n",
      "\n",
      "--- Applying L1 unstructured pruning (amount=0.5) ---\n",
      "Pruning hook has been applied.\n",
      "The layer now has a 'weight_mask' and 'weight_orig' attribute.\n",
      "\n",
      "--- Making pruning permanent for 'model.layers.0.mlp.gate_proj.weight' ---\n",
      "Pruning has been made permanent. The 'weight' attribute is now the sparse tensor.\n",
      "Sparsity of 'model.layers.0.mlp.gate_proj.weight' AFTER pruning: 50.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Unsupervised prunings\n",
      "Avg Latency: 1.257s\n",
      "P99 Latency: 1.400s\n",
      "Throughput: 0.80 samples/sec\n",
      "Max GPU Memory: 5124.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.06384811680437281, 'rouge2': 0.0, 'rougeL': 0.05183219399724151, 'rougeLsum': 0.05374848337669946}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.06384811680437281,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.05183219399724151,\n",
       " 'rougeLsum': 0.05374848337669946}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def prune_model_weights(model, amount=0.3):\n",
    "#     \"\"\"TODO: Applies L1 unstructured pruning to the linear layers of a model.\"\"\"\n",
    "#     print(f\"\\n--- Accessing Target Layer: {TARGET_LAYER_NAME_STR} ---\")\n",
    "#     target_module = get_module_by_name_str(model, TARGET_LAYER_NAME_STR)\n",
    "#     print(f\"Successfully accessed target layer of type: {type(target_module)}\")\n",
    "\n",
    "#     sparsity_before = calculate_sparsity(target_module, 'weight')\n",
    "#     print(f\"Sparsity of '{TARGET_LAYER_NAME_STR}.weight' BEFORE pruning: {sparsity_before:.2f}%\\n\")\n",
    "#     print(f\"--- Applying L1 unstructured pruning (amount={PRUNING_AMOUNT}) ---\")\n",
    "#     prune.l1_unstructured(target_module, name=\"weight\", amount=PRUNING_AMOUNT)\n",
    "\n",
    "#     print(\"Pruning hook has been applied.\")\n",
    "#     print(f\"The layer now has a 'weight_mask' and 'weight_orig' attribute.\")\n",
    "#     print(f\"\\n--- Making pruning permanent for '{TARGET_LAYER_NAME_STR}.weight' ---\")\n",
    "#     prune.remove(target_module, \"weight\")\n",
    "#     print(\"Pruning has been made permanent. The 'weight' attribute is now the sparse tensor.\")\n",
    "#     sparsity_after = calculate_sparsity(target_module, 'weight')\n",
    "#     print(f\"Sparsity of '{TARGET_LAYER_NAME_STR}.weight' AFTER pruning: {sparsity_after:.2f}%\\n\")\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model_prune = prune_model_weights(model_new,PRUNING_AMOUNT)\n",
    "# sample_texts = articles[:num_prediction]\n",
    "# generated, times = generate_headline(model_prune, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "# report_metrics(times,\" Unsupervised pruning\")\n",
    "# evaluate_model(datasets,generated,num_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model...\n",
      "Model loaded successfully and moved to device.\n",
      "Loaded 'strategic_pruned_model' model.\n",
      "Memory Footprint: 2357.13 MB\n"
     ]
    }
   ],
   "source": [
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Access a submodule in a model using its string name.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "def apply_pruning(model, layers_to_prune, amount, method):\n",
    "    \"\"\"Apply a specified pruning method to a list of layers.\"\"\"\n",
    "    parameters_to_prune = []\n",
    "    for layer_name in layers_to_prune:\n",
    "        try:\n",
    "            module = get_module_by_name(model, layer_name)\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Layer {layer_name} not found. Skipping.\")\n",
    "\n",
    "    if not parameters_to_prune:\n",
    "        print(\"No valid layers found to prune.\")\n",
    "        return\n",
    "\n",
    "    pruning_method_map = {\n",
    "        'l1_unstructured': prune.L1Unstructured,\n",
    "    }\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=pruning_method_map[method],\n",
    "        amount=amount,\n",
    "    )\n",
    "\n",
    "    # # Make the pruning permanent\n",
    "    # for module, param_name in parameters_to_prune:\n",
    "    #     prune.remove(module, param_name)\n",
    "    print(f\"Applied '{method}' pruning with {amount*100:.0f}% sparsity to {len(parameters_to_prune)} layers.\")\n",
    "dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "strategic_pruned_model,tokenizer = load_model(model_name=model_name,device=device)\n",
    "memory_baseline = get_model_memory_footprint(strategic_pruned_model)\n",
    "print(f\"Loaded '{'strategic_pruned_model'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_baseline:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj']\n",
      "['model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj']\n",
      "['model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj']\n",
      "['model.layers.0.mlp.gate_proj', 'model.layers.0.mlp.up_proj', 'model.layers.0.mlp.down_proj', 'model.layers.1.mlp.gate_proj', 'model.layers.1.mlp.up_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.2.mlp.up_proj', 'model.layers.2.mlp.down_proj', 'model.layers.3.mlp.gate_proj', 'model.layers.3.mlp.up_proj', 'model.layers.3.mlp.down_proj']\n"
     ]
    }
   ],
   "source": [
    "NUM_LAYERS_TO_TARGET = 4\n",
    "MLP_LAYERS = []\n",
    "for i in range(NUM_LAYERS_TO_TARGET):\n",
    "    MLP_LAYERS.extend([\n",
    "        f\"model.layers.{i}.mlp.gate_proj\",\n",
    "        f\"model.layers.{i}.mlp.up_proj\",\n",
    "        f\"model.layers.{i}.mlp.down_proj\"\n",
    "    ])\n",
    "    print(MLP_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 'l1_unstructured' pruning with 50% sparsity to 12 layers.\n"
     ]
    }
   ],
   "source": [
    "apply_pruning(strategic_pruned_model,MLP_LAYERS,PRUNING_AMOUNT,'l1_unstructured')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Magnitude Unstructured prunings\n",
      "Avg Latency: 1.203s\n",
      "P99 Latency: 1.901s\n",
      "Throughput: 0.83 samples/sec\n",
      "Max GPU Memory: 10860.50 MB\n",
      "ROUGE Scores: {'rouge1': 0.04866679337142641, 'rouge2': 0.0, 'rougeL': 0.04172254407577583, 'rougeLsum': 0.04151666104238316}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.04866679337142641,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.04172254407577583,\n",
       " 'rougeLsum': 0.04151666104238316}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(strategic_pruned_model, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "report_metrics(times,\" Magnitude Unstructured pruning\")\n",
    "evaluate_model(datasets,generated,NUM_PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "# Clean up model from memory\n",
    "clean(strategic_pruned_model,tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quantization-header"
   },
   "source": [
    "# 5. Model Compression: Quantization\n",
    "\n",
    "**Your Task:** Quantization reduces the precision of model weights (e.g., from 16-bit to 4-bit), significantly cutting down memory usage and often speeding up inference. You will define a 4-bit quantization configuration and use it to load and evaluate a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "quantization-code"
   },
   "outputs": [],
   "source": [
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"Calculates and returns the model's memory footprint in MB.\"\"\"\n",
    "    mem_params = sum(param.nelement() * param.element_size() for param in model.parameters())\n",
    "    mem_bufs = sum(buf.nelement() * buf.element_size() for buf in model.buffers())\n",
    "    total_mem_bytes = mem_params + mem_bufs\n",
    "    return total_mem_bytes / (1024 ** 2) # Convert bytes to MB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model...\n",
      "Model loaded successfully and moved to device.\n",
      "Loaded 'baseline_name' model.\n",
      "Memory Footprint: 2357.13 MB\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float16 \n",
    "\n",
    "quantized_model,tokenizer = load_model(model_name=model_name,device=device)\n",
    "memory_footprints = {}\n",
    "memory_baseline = get_model_memory_footprint(quantized_model)\n",
    "memory_footprints[\"baseline_name\"] = f\"{memory_baseline:.2f} MB\"\n",
    "print(f\"Loaded '{'baseline_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_baseline:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n",
      "Loaded 'quant_8bit_name' model.\n",
      "Memory Footprint: 1429.13 MB\n"
     ]
    }
   ],
   "source": [
    "clean(quantized_model,tokenizer)\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        quantization_config=quantization_config, \n",
    "        device_map=\"auto\" # Recommended for bitsandbytes\n",
    "    )\n",
    "memory_8bit = get_model_memory_footprint(model_8bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'quant_8bit_name' model.\n",
      "Memory Footprint: 1429.13 MB\n"
     ]
    }
   ],
   "source": [
    "memory_footprints[\"quant_8bit_name\"] = f\"{memory_8bit:.2f} MB\"\n",
    "print(f\"Loaded '{'quant_8bit_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_8bit:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Quantised 8bit models\n",
      "Avg Latency: 3.608s\n",
      "P99 Latency: 4.010s\n",
      "Throughput: 0.28 samples/sec\n",
      "Max GPU Memory: 8505.96 MB\n",
      "ROUGE Scores: {'rouge1': 0.0596551604580485, 'rouge2': 0.0, 'rougeL': 0.04445310216446986, 'rougeLsum': 0.05077744129307699}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.0596551604580485,\n",
       " 'rouge2': 0.0,\n",
       " 'rougeL': 0.04445310216446986,\n",
       " 'rougeLsum': 0.05077744129307699}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_8bit, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "report_metrics(times,\" Quantised 8bit model\")\n",
    "evaluate_model(datasets,generated,NUM_PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "clean(model_8bit,tokenizer)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        quantization_config=quantization_config, \n",
    "        device_map=\"auto\" # Recommended for bitsandbytes\n",
    "    )\n",
    "memory_4bit = get_model_memory_footprint(model_4bit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'quant_4bit_name' model.\n",
      "Memory Footprint: 965.13 MB\n"
     ]
    }
   ],
   "source": [
    "memory_footprints[\"quant_4bit_name\"] = f\"{memory_4bit:.2f} MB\"\n",
    "print(f\"Loaded '{'quant_4bit_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_4bit:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Quantised 4bit models\n",
      "Avg Latency: 1.653s\n",
      "P99 Latency: 1.734s\n",
      "Throughput: 0.60 samples/sec\n",
      "Max GPU Memory: 8505.96 MB\n",
      "ROUGE Scores: {'rouge1': 0.07217188820304037, 'rouge2': 0.005100714749837557, 'rougeL': 0.062138023446606995, 'rougeLsum': 0.06732401331757207}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.07217188820304037,\n",
       " 'rouge2': 0.005100714749837557,\n",
       " 'rougeL': 0.062138023446606995,\n",
       " 'rougeLsum': 0.06732401331757207}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_4bit, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "report_metrics(times,\" Quantised 4bit model\")\n",
    "evaluate_model(datasets,generated,NUM_PREDICTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "distributed-header"
   },
   "source": [
    "# 6. Distributed Inference (Multi-GPU)\n",
    "\n",
    "**Your Task:** If you have multiple GPUs, you can split the model across them to reduce the memory burden on a single GPU and potentially improve latency. We will explore two common techniques: Tensor Parallelism and Pipeline Parallelism.\n",
    "\n",
    "*Note: This section requires a multi-GPU environment.*\n",
    "\n",
    "### Tensor Parallelism\n",
    "Tensor parallelism splits individual model layers (the tensors) across multiple GPUs. Operations like matrix multiplications are executed in parallel on different GPUs, and the results are aggregated. This is highly effective for reducing the memory footprint of very large layers. The `accelerate` library can handle this automatically via `device_map=\"auto\"`.\n",
    "\n",
    "### Pipeline Parallelism\n",
    "Pipeline parallelism assigns entire layers or blocks of layers to different GPUs, creating a sequence or \"pipeline\" that the data flows through. For example, layers 1-10 run on GPU 0, layers 11-20 run on GPU 1, and so on. This is useful for very deep models where even a single layer might be too large for one GPU after tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0\n",
      "CUDA is available: True\n",
      "Number of GPUs available: 1\n",
      "!! WARNING: This demo is designed for 4 GPUs. It may not run correctly. !!\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "#print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() < 4:\n",
    "        print(\"!! WARNING: This demo is designed for 4 GPUs. It may not run correctly. !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.18.1.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-4h6e20g3/deepspeed_ab1919b42ee740678ccc4f2482e01473/setup.py\", line 110, in <module>\n",
      "  \u001b[31m   \u001b[0m     cuda_major_ver, cuda_minor_ver = installed_cuda_version()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-4h6e20g3/deepspeed_ab1919b42ee740678ccc4f2482e01473/op_builder/builder.py\", line 51, in installed_cuda_version\n",
      "  \u001b[31m   \u001b[0m     raise MissingCUDAException(\"CUDA_HOME does not exist, unable to compile CUDA op(s)\")\n",
      "  \u001b[31m   \u001b[0m op_builder.builder.MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed\n",
    "https://github.com/Surveshchauhan/LLM-Inference.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "distributed-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Check for multi-GPU environment and evaluate with Tensor Parallelism.\n",
    "# The `device_map=\"auto\"` in your `load_model` function should automatically apply this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate with Pipeline Parallelism.\n",
    "# This is more advanced and may require manually defining a device_map to assign\n",
    "# different layers of the model to different GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "speculative-header"
   },
   "source": [
    "# 7. Advanced Decoding: Speculative Decoding\n",
    "\n",
    "**Your Task:** Speculative decoding uses a smaller, faster \"draft\" model to generate several candidate tokens. A larger, more accurate \"target\" model then verifies these tokens in a single forward pass. This can significantly speed up generation if the draft model is a good predictor. You will load a larger target model and a smaller draft model, benchmark the target model alone, and then benchmark it with assistance from the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "speculative-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement and evaluate speculative decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "report-header"
   },
   "source": [
    "# 8. Final Report and Analysis\n",
    "\n",
    "**Your Task:** Consolidate your findings into a summary report. \n",
    "\n",
    "1.  Fill in the Markdown table below with the **Latency**, **Throughput**, and **ROUGE scores** for each optimization technique you implemented.\n",
    "2. Compile the final Project Report in PDF format:\n",
    "    *   Document the entire process, detailing the methodology, techniques, and libraries used.\n",
    "    *   Present the final benchmark results clearly.\n",
    "    *   Provide a thorough analysis of the trade-offs between performance, resources, and quality for each optimization step.\n",
    "    *   Conclude with recommendations for the most effective optimization strategy for this specific headline generation task, supported by your data.\n",
    "\n",
    "Some example questions for discussing the trade-offs:\n",
    "    *   Which method gave the best performance improvement?\n",
    "    *   Did any methods significantly hurt the ROUGE score (quality)?\n",
    "    *   Which optimization would you recommend for deployment in a production environment at the news portal, and why? Consider factors like cost, complexity, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "report-table"
   },
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Optimization Technique | Mean Latency (s) | Throughput (tokens/s) | ROUGE-1 Score |\n",
    "|--------------------------|------------------|-----------------------|---------------|\n",
    "| Baseline (No Cache)      | TODO             | TODO                  | TODO          |\n",
    "| KV Caching               | TODO             | TODO                  | TODO          |\n",
    "| Pruning (30%)            | TODO             | TODO                  | TODO          |\n",
    "| Quantization (4-bit)     | TODO             | TODO                  | TODO          |\n",
    "| Tensor Parallelism       | TODO             | TODO                  | TODO          |\n",
    "| Pipeline Parallelism     | TODO             | TODO                  | TODO          |\n",
    "| Speculative Decoding     | TODO             | TODO                  | TODO          |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
