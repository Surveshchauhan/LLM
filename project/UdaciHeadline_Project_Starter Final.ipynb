{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro-header"
   },
   "source": [
    "# UdaciHeadline: LLM Inference Optimization Project\n",
    "\n",
    "## Project Introduction\n",
    "Large Language Models (LLMs) are transforming content creation, but deploying them efficiently remains a major hurdle. Imagine you're an ML Engineer at a bustling online news portal. Your key task? Automatically generating catchy headlines from article summaries using an LLM. The problem? The current inference process is sluggish, causing publication delays and driving up operational costs. In this project, UdaciHeadline, you'll step into this role and tackle this critical challenge head-on. Your mission is to accelerate the headline generation pipeline significantly by applying state-of-the-art LLM inference optimization techniques. Get ready to dive deep into practical optimization and deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "## Project Summary\n",
    "This project provides hands-on experience in optimizing the inference performance of a pre-trained Large Language Model (like Llama-3.2-1B) for news headline generation. You will bring together concepts of LLM architecture, optimization techniques, and deployment frameworks. Specifically, you will:\n",
    "\n",
    "1.  **Establish a baseline** inference pipeline and profile its performance.\n",
    "2.  Implement and evaluate architectural optimizations like **KV-caching**.\n",
    "3.  Apply model compression techniques like **quantization** and **pruning**.\n",
    "4.  Configure and benchmark **distributed inference** using Tensor and Pipeline Parallelism.\n",
    "5.  Apply advanced decoding mechanisms like **speculative decoding**.\n",
    "6.  Perform comprehensive **benchmarking and analysis** across all stages.\n",
    "7.  Produce a **final report** summarizing findings and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports-header"
   },
   "source": [
    "## Install and Imports and Global Configuration\n",
    "\n",
    "Let's import the libraries we'll use throughout the project and define some constants like the model name and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\n",
      "Collecting huggingface-hub>=0.7.0 (from evaluate)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Collecting httpx<1.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=2.0.0->evaluate)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.7.0->evaluate)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, multiprocess, httpcore, hf-xet, huggingface-hub, httpx, datasets, evaluate\n",
      "\u001b[2K  Attempting uninstall: multiprocess\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/8\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18━━━━━━━━━━━\u001b[0m \u001b[32m1/8\u001b[0m [multiprocess]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [evaluate]6/8\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.3.0 evaluate-0.4.6 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 multiprocess-0.70.16 xxhash-3.6.0\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m219.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m175.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting absl-py (from rouge_score)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rouge_score) (3.9.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk->rouge_score) (4.67.1)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=82575f412ef7f053a1fbe3b5b87cd04452ad0fa6b0c8aa352566486745322785\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: absl-py, rouge_score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [rouge_score]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 rouge_score-0.1.2\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting torch<3,>=2.3 (from bitsandbytes)\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bitsandbytes) (24.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch<3,>=2.3->bitsandbytes)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m187.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m145.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m169.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [bitsandbytes][0m [bitsandbytes]er-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bitsandbytes-0.48.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.9.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n",
      "Requirement already satisfied: datasets in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip install --upgrade transformers\n",
    "!pip install rouge_score\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from evaluate import load as load_metric\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from time import time as get_time\n",
    "from pprint import pprint\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "import torch\n",
    "from time import time as get_time\n",
    "from huggingface_hub import login\n",
    "#os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# ---- Constants ----\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "MAX_NEW_TOKENS = 25 # Max length for the generated headline\n",
    "\n",
    "#PROMPT = \\\n",
    "# print(f\"Prompt: \\\"{PROMPT}\\\"\")\n",
    "TARGET_LAYER_NAME_STR = \"model.layers.0.mlp.gate_proj\"\n",
    "\n",
    "# We will prune 50% of the weights in this layer\n",
    "PRUNING_AMOUNT = 0.5\n",
    "NUM_PREDICTION = 50\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "df_results = pd.DataFrame(columns=['Optimization Technique ','Avg Latency','P99 Latency','Throughput','Max GPU Memory','rouge1','rouge2','rougeL','rougeLsum'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading-header"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We will use the \"News Category Dataset\" from Kaggle. The `kagglehub` library makes it easy to download and access. Your task is to implement the function to load and preprocess the data according to the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "data-loading-code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_news_dataset(path):\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files=path, split=\"train[:1000]\")\n",
    "    articles = [item[\"short_description\"] for item in dataset]\n",
    "    headlines = [item[\"headline\"] for item in dataset]\n",
    "\n",
    "    return dataset,articles,headlines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "We will define all the functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "baseline-helpers"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, quantization_config=None,device=\"cpu\"):\n",
    "\n",
    "    dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "    print(f\"Using model: {model_name}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(f\"Using dtype: {dtype}\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\",quantization_config=quantization_config).to(device)\n",
    "    print(\"Model loaded successfully and moved to device.\")\n",
    "    model.eval()\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    return model,tokenizer\n",
    "\n",
    "def generate_headline(model, tokenizer,texts,device,max_length,use_cache=False):\n",
    "\n",
    "    headlines = []\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            prompt = f\"Generate a concise headline for the following news summary:\\n{text}\"\n",
    "            prompt = f\"Short description: {text}\\nHeadline:\"\n",
    "            #print(prompt)\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "            start = get_time()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                use_cache=use_cache,  # Baseline: no KV caching\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            end = get_time()\n",
    "            latencies.append(end - start)\n",
    "            generated_text = tokenizer.decode(outputs[0] , skip_special_tokens=True)\n",
    "\n",
    "            headline_start = generated_text.find(\"Headline:\") + len(\"Headline:\")\n",
    "            headline_new = generated_text[headline_start:].strip()\n",
    "\n",
    "            headline_end = headline_new.find(\"Short description:\")\n",
    "            headline = headline_new[:headline_end].strip()\n",
    "\n",
    "            headlines.append(headline)\n",
    "    return headlines, latencies\n",
    "def generate_headline_quantized(model, tokenizer, texts, device_map, max_length, use_cache=False):\n",
    "\n",
    "\n",
    "    headlines = []\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            prompt = f\"Short description: {text}\\nHeadline:\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            first_device = next(model.parameters()).device\n",
    "            inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "\n",
    "            start = get_time()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                use_cache=use_cache,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "            end = get_time()\n",
    "            latencies.append(end - start)\n",
    "\n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Robust headline extraction\n",
    "            headline = generated_text.split(\"Headline:\")[-1].strip().split(\"Short description:\")[0].strip()\n",
    "            headlines.append(headline)\n",
    "\n",
    "    return headlines, latencies\n",
    "\n",
    "\n",
    "def report_metrics(times,section):\n",
    "    avg_latency = sum(times) / len(times)\n",
    "    p99_latency = sorted(times)[int(0.99 * len(times))]\n",
    "    throughput = len(times) / sum(times)\n",
    "    gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    print(f\"Below are for: {section}s\")\n",
    "    print(f\"Avg Latency: {avg_latency:.3f}s\")\n",
    "    print(f\"P99 Latency: {p99_latency:.3f}s\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "    print(f\"Max GPU Memory: {gpu_memory:.2f} MB\")\n",
    "\n",
    "    list_metric = [section,avg_latency,p99_latency,throughput,gpu_memory]\n",
    "    return list_metric\n",
    "\n",
    "def evaluate_model(dataset,generated,num_prediction):\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    references = [item for item in dataset[:num_prediction]['headline']]\n",
    "    results = rouge.compute(predictions=generated, references=references)\n",
    "    print(\"ROUGE Scores:\", results)\n",
    "    \n",
    "    return results\n",
    "def clean(model,tokenizer):\n",
    "    # Clean up model from memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\nCleaned up models and emptied CUDA cache.\")\n",
    "    def calculate_sparsity(module, param_name='weight'):\n",
    "    \"\"\"Calculates sparsity of a named parameter in a module.\"\"\"\n",
    "    if hasattr(module, param_name):\n",
    "        param = getattr(module, param_name)\n",
    "        if param is not None:\n",
    "            return 100. * float(torch.sum(param == 0)) / float(param.nelement())\n",
    "    return 0.0\n",
    "def print_headline(generated, headlines, articles,n=3):\n",
    "    for i in range(0,n):\n",
    "        print(f\"Generated Headline: \",generated[i])\n",
    "        print(f\"Actual Headline: \",headlines[i])\n",
    "        print(f\"Short Description: \",articles[i])\n",
    "        print(f\"\\n\")\n",
    "\n",
    "def get_module_by_name(model, module_name):\n",
    "    \"\"\"Access a submodule in a model using its string name.\"\"\"\n",
    "    names = module_name.split('.')\n",
    "    module = model\n",
    "    for name in names:\n",
    "        module = getattr(module, name)\n",
    "    return module\n",
    "    \n",
    "def apply_pruning(model, layers_to_prune, amount, method):\n",
    "    \"\"\"Apply a specified pruning method to a list of layers.\"\"\"\n",
    "    parameters_to_prune = []\n",
    "    for layer_name in layers_to_prune:\n",
    "        try:\n",
    "            module = get_module_by_name(model, layer_name)\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "        except AttributeError:\n",
    "            print(f\"Warning: Layer {layer_name} not found. Skipping.\")\n",
    "\n",
    "    if not parameters_to_prune:\n",
    "        print(\"No valid layers found to prune.\")\n",
    "        return\n",
    "\n",
    "    pruning_method_map = {\n",
    "        'l1_unstructured': prune.L1Unstructured,\n",
    "    }\n",
    "    \n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=pruning_method_map[method],\n",
    "        amount=amount,\n",
    "    )\n",
    "\n",
    "    # # Make the pruning permanent\n",
    "    # for module, param_name in parameters_to_prune:\n",
    "    #     prune.remove(module, param_name)\n",
    "    #print(f\"Applied '{method}' pruning with {amount*100:.0f}% sparsity to {len(parameters_to_prune)} layers.\")\n",
    "\n",
    "def get_model_memory_footprint(model):\n",
    "    \"\"\"Calculates and returns the model's memory footprint in MB.\"\"\"\n",
    "    mem_params = sum(param.nelement() * param.element_size() for param in model.parameters())\n",
    "    mem_bufs = sum(buf.nelement() * buf.element_size() for buf in model.buffers())\n",
    "    total_mem_bytes = mem_params + mem_bufs\n",
    "    return total_mem_bytes / (1024 ** 2) # Convert bytes to MB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Huggingface token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HF_TOKEN = ''#Add your token here\n",
    "login(token=HF_TOKEN)\n",
    "datasets, articles,headlines = load_news_dataset(\"../dataset/News_Category_Dataset.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline-header"
   },
   "source": [
    "# 2. Baseline Performance\n",
    "\n",
    "Before we can optimize, we need a starting point. Here, Lets establish the baseline performance of the `Llama-3.2-1B` model without any specific optimizations. We will measure latency, throughput, and the quality of the generated headlines using the ROUGE score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "baseline-eval"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully and moved to device.\n",
      "Below are for: Baselines\n",
      "Avg Latency: 1.754s\n",
      "P99 Latency: 2.098s\n",
      "Throughput: 0.57 samples/sec\n",
      "Max GPU Memory: 2372.75 MB\n",
      "ROUGE Scores: {'rouge1': 0.19725660191058997, 'rouge2': 0.0711636938654821, 'rougeL': 0.17296930902739638, 'rougeLsum': 0.1739549789328416}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.57013</td>\n",
       "      <td>2372.75</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                Baseline     1.753986     2.097501     0.57013   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0         2372.75  0.197257  0.071164  0.172969   0.173955  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "model_new,tokenizer = load_model(model_name=model_name,quantization_config = None,device=device)\n",
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_new, tokenizer, sample_texts, device,MAX_NEW_TOKENS)\n",
    "dict_ = report_metrics(times,\"Baseline\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccine\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  2 dead, 1 injured in plane crash in California\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "The dog is a very important part of the famil\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Moderate latency and low throughput. Acts as the control for all other techniques.\n",
    "• Memory: Efficient and fits comfortably on a single GPU.\n",
    "• Quality: ROUGE scores are decent, showing that the model can generate reasonable headlines without optimization.\n",
    "• Summary: A reliable starting point. Good balance of quality and resource usage, but slower than optimized variants\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kv-cache-header"
   },
   "source": [
    "# 3. Architectural Optimization: KV Caching\n",
    "\n",
    "One of the most effective ways to speed up token generation is using a Key-Value (KV) cache. This avoids re-computing attention scores for tokens that are already part of the sequence. \n",
    "Lets implement and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kv-cache-code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for: KV Cachings\n",
      "Avg Latency: 0.712s\n",
      "P99 Latency: 0.744s\n",
      "Throughput: 1.40 samples/sec\n",
      "Max GPU Memory: 2372.75 MB\n",
      "ROUGE Scores: {'rouge1': 0.19640260298652956, 'rouge2': 0.07316621056341521, 'rougeL': 0.1746354099929494, 'rougeLsum': 0.17505680921981875}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.75</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.75</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                Baseline     1.753986     2.097501    0.570130   \n",
       "1              KV Caching     0.712174     0.743731    1.404151   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0         2372.75  0.197257  0.071164  0.172969   0.173955  \n",
       "1         2372.75  0.196403  0.073166  0.174635   0.175057  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_new, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\"KV Caching\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccine\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  2 dead, 1 injured in plane crash in California\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "The dog is a very important part of the famil\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Dramatically reduces latency by avoiding redundant computation during autoregressive decoding. The model reuses previously computed attention keys and values, which speeds up generation without changing the output.\n",
    "• Memory: No additional memory savings, but no increase either  it’s a purely computational optimization.\n",
    "• Quality: ROUGE scores remain stable or slightly improve due to faster decoding and reduced token delay.\n",
    "• Summary: A low-risk, high-reward optimization. Ideal for any deployment scenario where latency matters.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "clean(model_new,tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pruning-header"
   },
   "source": [
    "# 4. Model Compression: Pruning\n",
    "\n",
    "Pruning removes redundant model weights, which can reduce model size and potentially speed up inference. Lets implement unstructured, magnitude-based pruning for MLP layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model...\n",
      "Model loaded successfully and moved to device.\n",
      "Loaded 'strategic_pruned_model' model.\n",
      "Memory Footprint: 2357.13 MB\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.bfloat16 if device == \"cuda\" and torch.cuda.is_bf16_supported() else torch.float32\n",
    "strategic_pruned_model,tokenizer = load_model(model_name=model_name,device=device)\n",
    "memory_baseline = get_model_memory_footprint(strategic_pruned_model)\n",
    "print(f\"Loaded '{'strategic_pruned_model'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_baseline:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS_TO_TARGET = 4\n",
    "MLP_LAYERS = []\n",
    "for i in range(NUM_LAYERS_TO_TARGET):\n",
    "    MLP_LAYERS.extend([\n",
    "        f\"model.layers.{i}.mlp.gate_proj\",\n",
    "        f\"model.layers.{i}.mlp.up_proj\",\n",
    "        f\"model.layers.{i}.mlp.down_proj\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_pruning(strategic_pruned_model,MLP_LAYERS,PRUNING_AMOUNT,'l1_unstructured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Magnitude Unstructured prunings\n",
      "Avg Latency: 0.599s\n",
      "P99 Latency: 0.625s\n",
      "Throughput: 1.67 samples/sec\n",
      "Max GPU Memory: 10868.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.14577913438561244, 'rouge2': 0.0327597067539767, 'rougeL': 0.13061666400632255, 'rougeLsum': 0.13089770273333565}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(strategic_pruned_model, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" Magnitude Unstructured pruning\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "# Clean up model from memory\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  The U.S. ordered 1.7 billion doses of the new boosters, which are 80% effective, fo\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  The 20-year-old man was arrested on suspicion of the murder of his 19-year-old girlfriend, who was killed i\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Improves latency and throughput by removing low-magnitude weights from linear layers. This reduces the number of active parameters and speeds up matrix operations.\n",
    "• Memory: Significant reduction in memory usage due to fewer active weights.\n",
    "• Quality: ROUGE scores drop sharply, indicating that pruning harms headline generation quality. \n",
    "• Summary: Best suited for extreme resource-constrained environments where speed is critical and quality can be sacrificed. Not recommended for production headline generation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "clean(strategic_pruned_model,tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "quantization-header"
   },
   "source": [
    "# 5. Model Compression: Quantization\n",
    "\n",
    "Quantization reduces the precision of model weights (e.g., from 16-bit to 4-bit), significantly cutting down memory usage and often speeding up inference. Lets implement both 8-bit quantization and a 4-bit quantization configuration and load a model and observe the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: meta-llama/Llama-3.2-1B\n",
      "Using device: cuda\n",
      "Using dtype: torch.bfloat16\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model...\n",
      "Model loaded successfully and moved to device.\n",
      "Loaded 'baseline_name' model.\n",
      "Memory Footprint: 2357.13 MB\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float16 \n",
    "quantized_model,tokenizer = load_model(model_name=model_name,device=device)\n",
    "memory_footprints = {}\n",
    "memory_baseline = get_model_memory_footprint(quantized_model)\n",
    "memory_footprints[\"baseline_name\"] = f\"{memory_baseline:.2f} MB\"\n",
    "print(f\"Loaded '{'baseline_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_baseline:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8-bit Quantization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'quant_8bit_name' model.\n",
      "Memory Footprint: 1429.13 MB\n"
     ]
    }
   ],
   "source": [
    "clean(quantized_model,tokenizer)\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        quantization_config=quantization_config, \n",
    "        device_map=\"auto\" # Recommended for bitsandbytes\n",
    "    )\n",
    "memory_8bit = get_model_memory_footprint(model_8bit)\n",
    "memory_footprints[\"quant_8bit_name\"] = f\"{memory_8bit:.2f} MB\"\n",
    "print(f\"Loaded '{'quant_8bit_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_8bit:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Quantised 8bit models\n",
      "Avg Latency: 1.807s\n",
      "P99 Latency: 1.960s\n",
      "Throughput: 0.55 samples/sec\n",
      "Max GPU Memory: 10868.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.20000103018514484, 'rouge2': 0.0763577855733028, 'rougeL': 0.16968265333091442, 'rougeLsum': 0.1712212066362102}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline_quantized(model_8bit, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" Quantised 8bit model\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccines for fall\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Man Who Flew From Plane To Escape Arrest Is Charged With Assault\"\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "Description: \"Until you have a dog you don\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Slightly slower than baseline due to quantization overhead, especially on CPUs or non-optimized GPU kernels.\n",
    "• Memory: Major memory savings  model weights occupy half the space of full precision.\n",
    "• Quality: ROUGE scores are stable or slightly better than baseline\n",
    "• Summary: Excellent choice for deployment on limited hardware. Should be combine with KV-caching for best results\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-bit Quantization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n",
      "Loaded 'quant_4bit_name' model.\n",
      "Memory Footprint: 965.13 MB\n"
     ]
    }
   ],
   "source": [
    "clean(model_8bit,tokenizer)\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        quantization_config=quantization_config, \n",
    "        device_map=\"auto\" # Recommended for bitsandbytes\n",
    "    )\n",
    "memory_4bit = get_model_memory_footprint(model_4bit)\n",
    "memory_footprints[\"quant_4bit_name\"] = f\"{memory_4bit:.2f} MB\"\n",
    "print(f\"Loaded '{'quant_4bit_name'}' model.\")\n",
    "print(f\"Memory Footprint: {memory_4bit:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Quantised 4bit models\n",
      "Avg Latency: 0.914s\n",
      "P99 Latency: 0.931s\n",
      "Throughput: 1.09 samples/sec\n",
      "Max GPU Memory: 10868.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.15255867871600398, 'rouge2': 0.04118585010548303, 'rougeL': 0.1347190371995811, 'rougeLsum': 0.1337735279968416}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantised 4bit model</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>1.094685</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "4             Quantised 4bit model     0.913505     0.930753    1.094685   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  \n",
       "4    10868.622559  0.152559  0.041186  0.134719   0.133774  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline_quantized(model_4bit, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" Quantised 4bit model\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  The U.S. has ordered 171 million doses of the new booster, which is expected to arrive in the fall. The\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  story: Airplane pilot arrested in Los Angeles\n",
      "The pilot of a small plane was arrested in Los Angeles on Friday, after\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "Description: \"Until you have a dog you don\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Faster than 8-bit and baseline due to smaller weight matrices and reduced memory bandwidth.\n",
    "• Memory: Extreme memory savings ideal for edge devices or low-cost GPU setups.\n",
    "• Quality: ROUGE scores drop moderately, indicating some loss of semantic richness and fluency.\n",
    "• Summary: A strong option for low-memory environments, but not ideal if headline quality is critical.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "clean(model_4bit,tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "distributed-header"
   },
   "source": [
    "# 6. Distributed Inference (Multi-GPU)\n",
    "\n",
    "*We are now goign to use  multiple GPUs, we will  split the model across them to reduce the memory burden on a single GPU and potentially improve latency. We will explore two common techniques: Tensor Parallelism and Pipeline Parallelism.\n",
    "\n",
    "*Note: This section requires a multi-GPU environment.*\n",
    "\n",
    "### Tensor Parallelism\n",
    "Tensor parallelism splits individual model layers (the tensors) across multiple GPUs. Operations like matrix multiplications are executed in parallel on different GPUs, and the results are aggregated. This is highly effective for reducing the memory footprint of very large layers. The `accelerate` library can handle this automatically via `device_map=\"auto\"`.\n",
    "\n",
    "### Pipeline Parallelism\n",
    "Pipeline parallelism assigns entire layers or blocks of layers to different GPUs, creating a sequence or \"pipeline\" that the data flows through. For example, layers 1-10 run on GPU 0, layers 11-20 run on GPU 1, and so on. This is useful for very deep models where even a single layer might be too large for one GPU after tensor parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA is available: True\n",
      "Number of GPUs available: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "#print(f\"DeepSpeed version: {deepspeed.__version__}\")\n",
    "print(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.device_count() < 4:\n",
    "        print(\"!! WARNING: This demo is designed for 4 GPUs. It may not run correctly. !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo ln -s /usr/local/cuda-12.2 /usr/local/cuda-11.8\n",
    "#uncomment to run if you face error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "distributed-code",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load model with automatic device mapping\n",
    "model_tp = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # Tensor parallelism\n",
    "    dtype=torch.float16\n",
    ")\n",
    "model_tp.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model_tp.config.pad_token_id = model_tp.config.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Tensor Parallels\n",
      "Avg Latency: 0.720s\n",
      "P99 Latency: 0.825s\n",
      "Throughput: 1.39 samples/sec\n",
      "Max GPU Memory: 10868.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.19586889007068448, 'rouge2': 0.0731644188873067, 'rougeL': 0.17310889396741813, 'rougeLsum': 0.1761237906089539}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantised 4bit model</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>1.094685</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tensor Parallel</td>\n",
       "      <td>0.720075</td>\n",
       "      <td>0.824669</td>\n",
       "      <td>1.388744</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "4             Quantised 4bit model     0.913505     0.930753    1.094685   \n",
       "5                  Tensor Parallel     0.720075     0.824669    1.388744   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  \n",
       "4    10868.622559  0.152559  0.041186  0.134719   0.133774  \n",
       "5    10868.622559  0.195869  0.073164  0.173109   0.176124  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_tp, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" Tensor Parallel\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n",
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccine\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  2 dead, 1 injured in plane crash in California\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "The dog is a great companion. He is\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean(model_tp,tokenizer)\n",
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Improves throughput and latency by splitting tensor operations across multiple GPUs. Each GPU handles part of the computation in parallel.\n",
    "• Memory: Efficient memory distribution across devices, allowing larger models to run without bottlenecks.\n",
    "• Quality: ROUGE scores remain stable, as the model architecture and weights are unchanged.\n",
    "• Summary: Ideal for multi-GPU setups. Offers strong performance gains without compromising quality.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "pipeline-code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  Pipeline Parallels\n",
      "Avg Latency: 0.719s\n",
      "P99 Latency: 0.729s\n",
      "Throughput: 1.39 samples/sec\n",
      "Max GPU Memory: 10868.62 MB\n",
      "ROUGE Scores: {'rouge1': 0.19586889007068448, 'rouge2': 0.0731644188873067, 'rougeL': 0.17310889396741813, 'rougeLsum': 0.1761237906089539}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantised 4bit model</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>1.094685</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tensor Parallel</td>\n",
       "      <td>0.720075</td>\n",
       "      <td>0.824669</td>\n",
       "      <td>1.388744</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pipeline Parallel</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.729432</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "4             Quantised 4bit model     0.913505     0.930753    1.094685   \n",
       "5                  Tensor Parallel     0.720075     0.824669    1.388744   \n",
       "6                Pipeline Parallel     0.719361     0.729432    1.390123   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  \n",
       "4    10868.622559  0.152559  0.041186  0.134719   0.133774  \n",
       "5    10868.622559  0.195869  0.073164  0.173109   0.176124  \n",
       "6    10868.622559  0.195869  0.073164  0.173109   0.176124  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"meta-llama/Llama-3.2-1B\",\n",
    "model_pp = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"balanced\",  # Pipeline parallelism\n",
    "    dtype=torch.float16\n",
    ")\n",
    "model_pp.eval()\n",
    "\n",
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(model_pp, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" Pipeline Parallel\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n",
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccine\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  2 dead, 1 injured in plane crash in California\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "The dog is a great companion. He is\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean(model_pp,tokenizer)\n",
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Similar to tensor parallelism, but splits model layers sequentially across GPUs. Each GPU handles a stage of the forward pass.\n",
    "• Memory: Enables deep models to run across devices, reducing per-GPU memory load.\n",
    "• Quality: Identical to tensor parallelism no change in output quality.\n",
    "• Summary: Best for very large models (e.g., 13B+) where layer depth exceeds single-GPU capacity. Slightly more complex to manage than tensor parallelism.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "speculative-header"
   },
   "source": [
    "# 7. Advanced Decoding: Speculative Decoding\n",
    "\n",
    "Speculative decoding uses a smaller, faster \"draft\" model to generate several candidate tokens. A larger, more accurate \"target\" model then verifies these tokens in a single forward pass. This can significantly speed up generation if the draft model is a good predictor. \n",
    "Lets load You will load a larger target model (meta-llama/Llama-3.1-8B) and a smaller draft model (meta-llama/Llama-3.2-1B), and benchmark the target model alone, and then benchmark it with assistance from the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a193cbbd1af4185ac2b15dd229cc0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "# Load Draft Model (1B)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "draft_model.eval()\n",
    "\n",
    "# Load Target Model (7B)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "target_model.eval()\n",
    "\n",
    "# Load ROUGE\n",
    "rouge = load_metric(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_speculative_decoding(texts, references, max_new_tokens=30):\n",
    "    predictions = []\n",
    "    latencies = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(target_model.device)\n",
    "            start = get_time()\n",
    "            outputs = target_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                use_cache=True,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                assistant_model=draft_model  # Enables speculative decoding\n",
    "            )\n",
    "            end = get_time()\n",
    "            latencies.append(end - start)\n",
    "            pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(pred)\n",
    "\n",
    "    # Metrics\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    p99_latency = sorted(latencies)[int(0.99 * len(latencies))]\n",
    "    throughput = len(latencies) / sum(latencies)\n",
    "    gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"\\n[Speculative Decoding]\")\n",
    "    print(f\"Avg Latency: {avg_latency:.3f}s\")\n",
    "    print(f\"P99 Latency: {p99_latency:.3f}s\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "    print(f\"Max GPU Memory: {gpu_memory:.2f} MB\")\n",
    "    print(\"ROUGE Scores:\", rouge_scores)\n",
    "    \n",
    "    list_metric = ['Speculative Decoding',avg_latency,p99_latency,throughput,gpu_memory]\n",
    "    return list_metric,rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Speculative Decoding]\n",
      "Avg Latency: 2.416s\n",
      "P99 Latency: 5.425s\n",
      "Throughput: 0.41 samples/sec\n",
      "Max GPU Memory: 13247.43 MB\n",
      "ROUGE Scores: {'rouge1': 0.13590574213264023, 'rouge2': 0.032221324839906175, 'rougeL': 0.10602704062087703, 'rougeLsum': 0.11043058201812977}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantised 4bit model</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>1.094685</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tensor Parallel</td>\n",
       "      <td>0.720075</td>\n",
       "      <td>0.824669</td>\n",
       "      <td>1.388744</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pipeline Parallel</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.729432</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Speculative Decoding</td>\n",
       "      <td>2.415523</td>\n",
       "      <td>5.424541</td>\n",
       "      <td>0.413989</td>\n",
       "      <td>13247.425293</td>\n",
       "      <td>0.135906</td>\n",
       "      <td>0.032221</td>\n",
       "      <td>0.106027</td>\n",
       "      <td>0.110431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "4             Quantised 4bit model     0.913505     0.930753    1.094685   \n",
       "5                  Tensor Parallel     0.720075     0.824669    1.388744   \n",
       "6                Pipeline Parallel     0.719361     0.729432    1.390123   \n",
       "7             Speculative Decoding     2.415523     5.424541    0.413989   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  \n",
       "4    10868.622559  0.152559  0.041186  0.134719   0.133774  \n",
       "5    10868.622559  0.195869  0.073164  0.173109   0.176124  \n",
       "6    10868.622559  0.195869  0.073164  0.173109   0.176124  \n",
       "7    13247.425293  0.135906  0.032221  0.106027   0.110431  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model_tp.config.pad_token_id = model_tp.config.eos_token_id\n",
    "\n",
    "\n",
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "references = [item for item in datasets[:NUM_PREDICTION]['headline']]\n",
    "dict_, res = run_speculative_decoding(sample_texts, references)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  \"U.S. to order 100 million more doses of COVID-19 vaccine\"\n",
      "Actual Headline:  Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters\n",
      "Short Description:  Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "\n",
      "\n",
      "Generated Headline:  2 dead, 1 injured in plane crash in California\n",
      "Actual Headline:  American Airlines Flyer Charged, Banned For Life After Punching Flight Attendant On Video\n",
      "Short Description:  He was subdued by passengers and crew when he fled to the back of the aircraft after the confrontation, according to the U.S. attorney's office in Los Angeles.\n",
      "\n",
      "\n",
      "Generated Headline:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "The dog is a great companion. He is\n",
      "Actual Headline:  23 Of The Funniest Tweets About Cats And Dogs This Week (Sept. 17-23)\n",
      "Short Description:  \"Until you have a dog you don't understand what could be eaten.\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_headline(generated, headlines, articles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Surprisingly slower in this setup due to mismatch between draft and target models. Latency and P99 latency are higher than baseline.\n",
    "• Memory: Requires both models in memory, leading to the highest GPU usage in the benchmark.\n",
    "• Quality: ROUGE scores are significantly lower, suggesting that the draft model’s predictions are frequently rejected or misaligned.\n",
    "• Summary: Promising technique in theory, but underperformed here due to poor draft-target synergy. May work better with smaller gaps between models or fine-tuned drafts.\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now run larger target model and check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are for:  target_models\n",
      "Avg Latency: 1.715s\n",
      "P99 Latency: 2.041s\n",
      "Throughput: 0.58 samples/sec\n",
      "Max GPU Memory: 13247.43 MB\n",
      "ROUGE Scores: {'rouge1': 0.20109832229004854, 'rouge2': 0.07571960213214383, 'rougeL': 0.1786265297163076, 'rougeLsum': 0.18017525964235714}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimization Technique</th>\n",
       "      <th>Avg Latency</th>\n",
       "      <th>P99 Latency</th>\n",
       "      <th>Throughput</th>\n",
       "      <th>Max GPU Memory</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>1.753986</td>\n",
       "      <td>2.097501</td>\n",
       "      <td>0.570130</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.197257</td>\n",
       "      <td>0.071164</td>\n",
       "      <td>0.172969</td>\n",
       "      <td>0.173955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KV Caching</td>\n",
       "      <td>0.712174</td>\n",
       "      <td>0.743731</td>\n",
       "      <td>1.404151</td>\n",
       "      <td>2372.750000</td>\n",
       "      <td>0.196403</td>\n",
       "      <td>0.073166</td>\n",
       "      <td>0.174635</td>\n",
       "      <td>0.175057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magnitude Unstructured pruning</td>\n",
       "      <td>0.598740</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>1.670175</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.145779</td>\n",
       "      <td>0.032760</td>\n",
       "      <td>0.130617</td>\n",
       "      <td>0.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantised 8bit model</td>\n",
       "      <td>1.806751</td>\n",
       "      <td>1.959691</td>\n",
       "      <td>0.553480</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.200001</td>\n",
       "      <td>0.076358</td>\n",
       "      <td>0.169683</td>\n",
       "      <td>0.171221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantised 4bit model</td>\n",
       "      <td>0.913505</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>1.094685</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.152559</td>\n",
       "      <td>0.041186</td>\n",
       "      <td>0.134719</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tensor Parallel</td>\n",
       "      <td>0.720075</td>\n",
       "      <td>0.824669</td>\n",
       "      <td>1.388744</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pipeline Parallel</td>\n",
       "      <td>0.719361</td>\n",
       "      <td>0.729432</td>\n",
       "      <td>1.390123</td>\n",
       "      <td>10868.622559</td>\n",
       "      <td>0.195869</td>\n",
       "      <td>0.073164</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.176124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Speculative Decoding</td>\n",
       "      <td>2.415523</td>\n",
       "      <td>5.424541</td>\n",
       "      <td>0.413989</td>\n",
       "      <td>13247.425293</td>\n",
       "      <td>0.135906</td>\n",
       "      <td>0.032221</td>\n",
       "      <td>0.106027</td>\n",
       "      <td>0.110431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>target_model</td>\n",
       "      <td>1.715131</td>\n",
       "      <td>2.041321</td>\n",
       "      <td>0.583046</td>\n",
       "      <td>13247.425293</td>\n",
       "      <td>0.201098</td>\n",
       "      <td>0.075720</td>\n",
       "      <td>0.178627</td>\n",
       "      <td>0.180175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Optimization Technique   Avg Latency  P99 Latency  Throughput  \\\n",
       "0                         Baseline     1.753986     2.097501    0.570130   \n",
       "1                       KV Caching     0.712174     0.743731    1.404151   \n",
       "2   Magnitude Unstructured pruning     0.598740     0.625304    1.670175   \n",
       "3             Quantised 8bit model     1.806751     1.959691    0.553480   \n",
       "4             Quantised 4bit model     0.913505     0.930753    1.094685   \n",
       "5                  Tensor Parallel     0.720075     0.824669    1.388744   \n",
       "6                Pipeline Parallel     0.719361     0.729432    1.390123   \n",
       "7             Speculative Decoding     2.415523     5.424541    0.413989   \n",
       "8                     target_model     1.715131     2.041321    0.583046   \n",
       "\n",
       "   Max GPU Memory    rouge1    rouge2    rougeL  rougeLsum  \n",
       "0     2372.750000  0.197257  0.071164  0.172969   0.173955  \n",
       "1     2372.750000  0.196403  0.073166  0.174635   0.175057  \n",
       "2    10868.622559  0.145779  0.032760  0.130617   0.130898  \n",
       "3    10868.622559  0.200001  0.076358  0.169683   0.171221  \n",
       "4    10868.622559  0.152559  0.041186  0.134719   0.133774  \n",
       "5    10868.622559  0.195869  0.073164  0.173109   0.176124  \n",
       "6    10868.622559  0.195869  0.073164  0.173109   0.176124  \n",
       "7    13247.425293  0.135906  0.032221  0.106027   0.110431  \n",
       "8    13247.425293  0.201098  0.075720  0.178627   0.180175  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = articles[:NUM_PREDICTION]\n",
    "generated, times = generate_headline(target_model, tokenizer, sample_texts, device,MAX_NEW_TOKENS,\"True\")\n",
    "dict_ = report_metrics(times,\" target_model\")\n",
    "res = evaluate_model(datasets,generated,NUM_PREDICTION)\n",
    "df_results.loc[len(df_results)] = (dict_+ list(res.values()))\n",
    "df_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "• Performance: Slower than baseline due to larger model size and deeper architecture.\n",
    "• Memory: Highest memory usage in the benchmark.\n",
    "• Quality: Best ROUGE scores across all variants  headlines are more fluent, informative, and accurate.\n",
    "• Summary: Ideal for quality-first applications. Use with parallelism or quantization to mitigate resource demands.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned up models and emptied CUDA cache.\n",
      "\n",
      "Cleaned up models and emptied CUDA cache.\n"
     ]
    }
   ],
   "source": [
    "clean(draft_model,tokenizer)\n",
    "clean(target_model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "report-header"
   },
   "source": [
    "# 8. Final Report and Analysis\n",
    "\n",
    "Final report is attached as PDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "report-table"
   },
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Optimization Technique              | Avg Latency | P99 Latency | Throughput | Max GPU Memory | ROUGE-1  | ROUGE-2  | ROUGE-L  | ROUGE-Lsum |\n",
    "|------------------------------------|-------------|-------------|------------|----------------|----------|----------|----------|-------------|\n",
    "| Baseline                           | 1.753986    | 2.097501    | 0.570130   | 2372.750000    | 0.197257 | 0.071164 | 0.172969 | 0.173955    |\n",
    "| KV Caching                         | 0.712174    | 0.743731    | 1.404151   | 2372.750000    | 0.196403 | 0.073166 | 0.174635 | 0.175057    |\n",
    "| Magnitude Unstructured Pruning     | 0.598740    | 0.625304    | 1.670175   | 10868.622559   | 0.145779 | 0.032760 | 0.130617 | 0.130898    |\n",
    "| Quantised 8bit Model               | 1.806751    | 1.959691    | 0.553480   | 10868.622559   | 0.200001 | 0.076358 | 0.169683 | 0.171221    |\n",
    "| Quantised 4bit Model               | 0.913505    | 0.930753    | 1.094685   | 10868.622559   | 0.152559 | 0.041186 | 0.134719 | 0.133774    |\n",
    "| Tensor Parallel                    | 0.720075    | 0.824669    | 1.388744   | 10868.622559   | 0.195869 | 0.073164 | 0.173109 | 0.176124    |\n",
    "| Pipeline Parallel                  | 0.719361    | 0.729432    | 1.390123   | 10868.622559   | 0.195869 | 0.073164 | 0.173109 | 0.176124    |\n",
    "| Speculative Decoding               | 2.415523    | 5.424541    | 0.413989   | 13247.425293   | 0.135906 | 0.032221 | 0.106027 | 0.110431    |\n",
    "| Target Model                       | 1.715131    | 2.041321    | 0.583046   | 13247.425293   | 0.201098 | 0.075720 | 0.178627 | 0.180175    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
